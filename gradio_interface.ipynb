{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4897f346-ac5d-4dfe-93b7-f1b01d005698",
   "metadata": {},
   "source": [
    "gardio interface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11e51e4-af87-4906-8d0d-f61aa54304c7",
   "metadata": {},
   "source": [
    "Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa6075c-3b5d-4280-b043-88661b9ff8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install ultralytics gradio opencv-python-headless transformers torch torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19195fd-5948-4c55-a1c3-b5a34273f651",
   "metadata": {},
   "source": [
    "Gradio interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b23bf70d-d33e-4966-b727-4daae92b5bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 (no detections), 417.6ms\n",
      "Speed: 43.9ms preprocess, 417.6ms inference, 17.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 305.9ms\n",
      "Speed: 10.9ms preprocess, 305.9ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 353.0ms\n",
      "Speed: 13.6ms preprocess, 353.0ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 Squash Powdery mildew leaf, 314.9ms\n",
      "Speed: 17.5ms preprocess, 314.9ms inference, 40.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 Apple Scab Leaf, 1 Squash Powdery mildew leaf, 293.5ms\n",
      "Speed: 12.6ms preprocess, 293.5ms inference, 21.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 (no detections), 284.8ms\n",
      "Speed: 18.4ms preprocess, 284.8ms inference, 4.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 Squash Powdery mildew leaf, 471.3ms\n",
      "Speed: 14.9ms preprocess, 471.3ms inference, 2.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 Squash Powdery mildew leaf, 1 grape leaf black rot, 287.8ms\n",
      "Speed: 32.5ms preprocess, 287.8ms inference, 4.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 1 Squash Powdery mildew leaf, 1 grape leaf black rot, 287.1ms\n",
      "Speed: 18.6ms preprocess, 287.1ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x480 (no detections), 271.5ms\n",
      "Speed: 14.4ms preprocess, 271.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 480)\n",
      "\n",
      "0: 640x640 (no detections), 295.5ms\n",
      "Speed: 10.0ms preprocess, 295.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x384 2 Bell_pepper leaf spots, 1 Bell_pepper leaf, 322.7ms\n",
      "Speed: 9.8ms preprocess, 322.7ms inference, 2.2ms postprocess per image at shape (1, 3, 640, 384)\n",
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import gradio as gr\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFilter\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from transformers import ViTForImageClassification\n",
    "import json\n",
    "\n",
    "# Load model for segmentation/blur task\n",
    "\n",
    "model = YOLO(r\"C:\\Users\\Lenovo\\Desktop\\external project\\best.pt\")\n",
    "\n",
    "\n",
    "# Load ViT model for classification\n",
    "vit_model_path = r\"C:\\Users\\Lenovo\\Desktop\\external project\\vit_model.pth\"\n",
    "vit_model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224-in21k\", num_labels=38)\n",
    "vit_model.load_state_dict(torch.load(vit_model_path, map_location=torch.device(\"cpu\")))\n",
    "vit_model.eval()\n",
    "\n",
    "# Load class names\n",
    "with open(r\"C:\\Users\\Lenovo\\Desktop\\external project\\class_names.json\", \"r\") as f:\n",
    "    class_names = json.load(f)\n",
    "\n",
    "# Image Transform for ViT\n",
    "IMG_SIZE = (224, 224)\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Rule-based chatbot\n",
    "def rule_based_chat(user_input):\n",
    "    user_input = user_input.lower()\n",
    "    if \"how to treat\" in user_input:\n",
    "        return \"Use organic pesticides and remove infected leaves.\"\n",
    "    elif \"symptoms\" in user_input:\n",
    "        return \"Common symptoms include yellowing, spots, and wilting.\"\n",
    "    elif \"prevention\" in user_input:\n",
    "        return \"Avoid overwatering and ensure good air circulation.\"\n",
    "    else:\n",
    "        return \"I'm still learning! Try asking about symptoms, treatment, or prevention.\"\n",
    "\n",
    "# Image processing with segmentation + ViT classification\n",
    "def full_pipeline(img):\n",
    "    image_np = np.array(img)\n",
    "\n",
    "    # Use for segmentation/blur task\n",
    "    results = model(image_np)\n",
    "\n",
    "    # Apply blur everywhere\n",
    "    pil_img = Image.fromarray(image_np)\n",
    "    blurred = pil_img.filter(ImageFilter.GaussianBlur(radius=10))\n",
    "\n",
    "    # Even if detects something or not, we classify the full image using ViT\n",
    "    input_tensor = valid_transform(pil_img).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        outputs = vit_model(input_tensor)\n",
    "        pred_idx = outputs.logits.argmax(dim=1).item()\n",
    "        disease_name = class_names[pred_idx]\n",
    "\n",
    "    if len(results[0].boxes) > 0:\n",
    "        # Use the first box for simplicity if YOLO detects a disease region\n",
    "        box = results[0].boxes[0]\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "\n",
    "        # Paste the clear region from the original image onto the blurred image\n",
    "        clear_patch = pil_img.crop((x1, y1, x2, y2))\n",
    "        blurred.paste(clear_patch, (x1, y1))\n",
    "\n",
    "    return np.array(blurred), disease_name\n",
    "\n",
    "# Gradio interface\n",
    "image_input = gr.Image(type=\"pil\")\n",
    "text_input = gr.Textbox(label=\"Ask a question (e.g., 'how to treat it?')\")\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=lambda img, text: [*full_pipeline(img), rule_based_chat(text)],\n",
    "    inputs=[image_input, text_input],\n",
    "    outputs=[gr.Image(label=\"Segmented Image\"), gr.Text(label=\"Predicted Disease\"), gr.Text(label=\"Bot Answer\")],\n",
    "    title=\"ðŸŒ¿ Plant Disease Detector & Assistant\",\n",
    "    description=\"Upload an image to detect disease using , then ask plant-care questions.\"\n",
    ")\n",
    "\n",
    "iface.launch(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a195744-7e29-49db-a5c7-f38b77a85399",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 (no detections), 420.1ms\n",
      "Speed: 15.9ms preprocess, 420.1ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "\n",
      "0: 640x640 2 Bell_pepper leaf spots, 2 Bell_pepper leafs, 501.0ms\n",
      "Speed: 33.0ms preprocess, 501.0ms inference, 4.3ms postprocess per image at shape (1, 3, 640, 640)\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import gradio as gr\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image, ImageFilter\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from transformers import ViTForImageClassification\n",
    "import json\n",
    "\n",
    "# Load model for segmentation/blur task\n",
    "model = YOLO(r\"C:\\Users\\Lenovo\\Desktop\\external project\\best.pt\")\n",
    "\n",
    "# Load ViT model for classification\n",
    "vit_model_path = r\"C:\\Users\\Lenovo\\Desktop\\external project\\vit_model.pth\"\n",
    "vit_model = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224-in21k\", num_labels=38)\n",
    "vit_model.load_state_dict(torch.load(vit_model_path, map_location=torch.device(\"cpu\")))\n",
    "vit_model.eval()\n",
    "\n",
    "# Load class names\n",
    "with open(r\"C:\\Users\\Lenovo\\Desktop\\external project\\class_names.json\", \"r\") as f:\n",
    "    class_names = json.load(f)\n",
    "\n",
    "# Image Transform for ViT\n",
    "IMG_SIZE = (224, 224)\n",
    "valid_transform = transforms.Compose([\n",
    "    transforms.Resize(IMG_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Rule-based chatbot\n",
    "def rule_based_chat(user_input):\n",
    "    user_input = user_input.lower()\n",
    "    if \"how to treat\" in user_input:\n",
    "        return \"Use organic pesticides and remove infected leaves.\"\n",
    "    elif \"symptoms\" in user_input:\n",
    "        return \"Common symptoms include yellowing, spots, and wilting.\"\n",
    "    elif \"prevention\" in user_input:\n",
    "        return \"Avoid overwatering and ensure good air circulation.\"\n",
    "    else:\n",
    "        return \"I'm still learning! Try asking about symptoms, treatment, or prevention.\"\n",
    "\n",
    "# Image processing with segmentation + ViT classification\n",
    "def full_pipeline(img):\n",
    "    # Resize input to 224x224\n",
    "    img_resized = img.resize((224, 224))\n",
    "    image_np = np.array(img_resized)\n",
    "\n",
    "    # Use for segmentation/blur task\n",
    "    results = model(image_np)\n",
    "\n",
    "    # Apply blur everywhere\n",
    "    blurred = img_resized.filter(ImageFilter.GaussianBlur(radius=10))\n",
    "\n",
    "    # ViT classification\n",
    "    input_tensor = valid_transform(img_resized).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        outputs = vit_model(input_tensor)\n",
    "        pred_idx = outputs.logits.argmax(dim=1).item()\n",
    "        disease_name = class_names[pred_idx]\n",
    "\n",
    "    if len(results[0].boxes) > 0:\n",
    "        # Use the first box detected by YOLO\n",
    "        box = results[0].boxes[0]\n",
    "        x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "\n",
    "        # Ensure box coordinates are within 224x224 bounds\n",
    "        x1 = max(0, min(223, x1))\n",
    "        y1 = max(0, min(223, y1))\n",
    "        x2 = max(0, min(224, x2))\n",
    "        y2 = max(0, min(224, y2))\n",
    "\n",
    "        # Paste the clear patch from original resized image\n",
    "        clear_patch = img_resized.crop((x1, y1, x2, y2))\n",
    "        blurred.paste(clear_patch, (x1, y1))\n",
    "\n",
    "    # Return the 224x224 processed image and disease prediction\n",
    "    return np.array(blurred), disease_name\n",
    "\n",
    "# Gradio interface\n",
    "image_input = gr.Image(type=\"pil\")\n",
    "text_input = gr.Textbox(label=\"Ask a question (e.g., 'how to treat it?')\")\n",
    "\n",
    "iface = gr.Interface(\n",
    "    fn=lambda img, text: [*full_pipeline(img), rule_based_chat(text)],\n",
    "    inputs=[image_input, text_input],\n",
    "    outputs=[gr.Image(label=\"Segmented Image\"), gr.Text(label=\"Predicted Disease\"), gr.Text(label=\"Bot Answer\")],\n",
    "    title=\"ðŸŒ¿ Plant Disease Detector & Assistant\",\n",
    "    description=\"Upload an image to detect disease using YOLO, then ask plant-care questions.\"\n",
    ")\n",
    "\n",
    "iface.launch(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925c6d69-92dc-447f-bf74-d606641038a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecaaa51-5323-4638-8012-de33d31dc276",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
